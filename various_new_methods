
#pending
-----------
statistics testign for a X features
https://www.youtube.com/watch?v=Ren6frLTCKU
-------------------------------------
https://github.com/123deepak/Machine-Learning/blob/master/Hierarical%20and%20TSNE%20Clustering%20over%20Grain%20data%20set%20and%20Visualizations.ipynb
Hierchial clustering

pd.crosstab(df['SFH'], df['Result'])
----------------------
y=df['Result']
X=df.drop(columns=['Result'], axis =1)
for col in X.columns:
    temp = pd.DataFrame()
    index = 0
    for val in df[col].unique():
        denominator = df[df[col] == val].shape[0]
        numerator = df[(df[col] == val) & (df['Result'] == 1)].shape[0]
        perc = numerator * 100 / denominator
        temp.loc[index, 'val'] = val
        temp.loc[index, 'perc'] = perc
        index = index + 1
    temp.sort_values(['val'], inplace = True)
    plt.figure()
    plt.plot(temp['val'], temp['perc'])
    plt.title("Distribution for {}".format(col))
    plt.xticks(df[col].unique())
    plt.legend(['% of 1 falling in each category'])
    plt.show()
    
    
    X = data.iloc[:, 0:-2]
y = data.iloc[:,-1]

#################################
############## check null values
df[df['Age'].isna()]
df['Age'].replace('32','None',inplace=True)
# train a Gaussian Naive Bayes classifier on the training set
from sklearn.naive_bayes import GaussianNB
# instantiate the model
gnb = GaussianNB()
# fit the model
gnb.fit(X_train, y_train)
y_pred = gnb.predict(X_test)

df.groupby(['Country'])['Sales'].plot.bar()
df1=df.groupby(['Country','CustomerID'])['Quantity].max()
df1 = df.set_index(['Country','CustomerID','InvoiceNo','Quantity','UnitPrice']).sort_index()
-------------------------------------------------------------------------
# check the details of data
df.shape
df.dtypes
df.isnull().sum()
df.columns

#drop the columns not required in one shot
df.drop(columns=['RowNumber', 'CustomerId', 'Surname'], axis =1,inplace=True)

import missingno as msno
msno.bar(data)
plt.show()

#show all categorical columns
for column in df.columns:
    if df[column].dtype == object and len(df[column].unique()) <= 50:
        categorical_col.append(column)
        print(f"{column} : {df[column].unique()}")
        print("====================================")
#one hot encoding
dummies = pd.get_dummies(df[['Gender','Geography']], drop_first=True)
df=pd.concat([df.drop(['Gender','Geography'],axis=1), dummies],axis=1)


# Rounding to 2 decimals for float and converting to int
df['Balance']=df['Balance'].round(0).astype(int)
df['EstimatedSalary']=df['EstimatedSalary'].round(0).astype(int)


#  check if the output feature(y) is imbalanced dataset
pd.value_counts(df['Survived'])

Pandas apply value_counts on multiple columns at once
The below example show how to apply Pandas method value_counts on multiple columns of a Dataframe ot once by using pandas.DataFrame.apply. 
This solution is working well for small to medium sized DataFrames. The syntax is simple - the first one is for the whole DataFrame:

df.apply(pd.Series.value_counts).head()

pip install pandas_profiling (in console)
from pandas_profiling import ProfileReport
### To Create the Simple report quickly
df=pd.read_csv("filename")
profile = ProfileReport(df, title='Pandas Profiling Report', explorative=True)
profile.to_widgets()

# drop a row if missing values are less percentage
df.dropna(subset=['Fare'],how='any',inplace=True)

#how to fill missing values column mean
df['Age'].fillna((df['Age'].mean()), inplace=True)

#pip install pandas_visual-analysis 
from pandas_visual_analysis import VisualAnalysis
VisualAnalysis(df)



https://github.com/iNeuronai/interview-question-data-science-/blob/master/Final%20Data%20Science%20Interview%20Questions(%23Day30).pdf
# if heavily imbalanced data in the dependent variable y in thyroid detection
x= new_data.drop(['class'],axis=1)
y=new_data['Class']
rdsample=RandomOverSampler()
x_sampled, y_sampled=rdsample.fit_sample(x,y)




KNNimputer()
https://pypi.org/project/autoimpute
https://readthedocs.org/projects/autoimpute/downloads/pdf/latest/
from sklearn.preprocessing import LabelEncoder
from autoimpute.imputations import SingleImputer, MultipleImputer
from autoimpute.imputations import MulitnomialLogisticImputer
#start the imputing the missing values. we will use autoimputer
impute=SingleImputer(strategy='categorial')
#here in this mushroom  features are all categorical
impute.fit(data)
new_data=impute.transform(data)
new_data








Handling categorical values
---------------------------

df1=pd.get_dummies(df['State'],drop_first=True)
df=pd.concat([df1,df],axis=1)
df.drop('State',axis=1,inplace=True)

What is an outlier?
------------------
An outlier is a data point in a data set that is distant from all other observations. A data point that lies outside the overall distribution of the dataset.
-------------

outliers=[]
def detect_outliers(data):
    
    threshold=3
    mean = np.mean(data)
    std =np.std(data)
    
    
    for i in data:
        z_score= (i - mean)/std 
        if np.abs(z_score) > threshold:
            outliers.append(y)
    return outliers
    
outlier_pt=detect_outliers(dataset)
outlier_pt

# skewed distributions
---------------------
for col in df.columns:
    print(col)
    print({df[col].skew()})
    

    
 # Visulazing the distibution of the data for every feature
df.hist(edgecolor='black', linewidth=1.2, figsize=(20, 20));   
    #from pandas_profiling import ProfileReport
    
 # Transform categorical data into dummies
# categorical_col.remove("Attrition")
# data = pd.get_dummies(df, columns=categorical_col)
# data.info()



from sklearn.preprocessing import LabelEncoder

label = LabelEncoder()
for column in categorical_col:
    df[column] = label.fit_transform(df[column])   
    
    # Transform categorical data into dummies
# categorical_col.remove("Attrition")
# data = pd.get_dummies(df, columns=categorical_col)
# data.info()


Codigo	Polaridade	VCE(V)	Ic(A)	Potencia(W)	hfe
0	BC107	NPN	45	0,1	0,3	                    110-450
1	BC108	NPN	20	0,1	0,3	                    110-800
2	BC109	NPN	20	0,1	0,3	                    200-800
3	BC177	PNP	45	0,1	0,3	                    75-260
4	BC178	PNP	25	0,1	0,3	                    125-500


di  = df["hfe"].str.split("-") 
df["hfe_min"] = di.str.get(0) 
df["hfe_max"] = di.str.get(1) 

df["hfe_max"] = df["hfe_max"].str.replace(">","") 

df = tabela.drop(["hfe"], axis=1) 


# let's see how data is distributed for every column
plt.figure(figsize=(20,25), facecolor='white')
plotnumber = 1

for column in data:
    if plotnumber<=16 :
        ax = plt.subplot(4,4,plotnumber)
        sns.distplot(data[column])
        plt.xlabel(column,fontsize=20)
        plt.ylabel('Salary',fontsize=20)
    plotnumber+=1
plt.tight_layout()

#################
 https://www.oracle.com/in/a/ocom/docs/data-science-lifecycle-ebook.pdf
df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data',
                 names=column_names, sep=',\s*', na_values='?')
test_df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test',
                      names=column_names, sep=',\s*', na_values='?', skiprows=1)
https://github.com/darenr/public_datasets/raw/master/churn_dataset.csv
https://docs.oracle.com/en-us/iaas/tools/ads-sdk/latest/user_guide/automl/automl.html#keras
fraud detection and churn prediction


#implement pie chart
labels = 'Exited', 'Retained'
sizes = [df.Exited[df['Exited']==1].count(), df.Exited[df['Exited']==0].count()]
explode = (0, 0.1)
fig1, ax1 = plt.subplots(figsize=(10, 8))
ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.axis('equal')
plt.title("Proportion of customer churned and retained", size = 20)
plt.show()

#Univariate Analysis
IRIS dataset
sepal length Vs  species(setosa,verginica,versica) individaually

#bi variate Analysis
sns.FacetGrid(df,hue="species",size=5).map(plt.scatter,"petallength","petalwideth").add_legend()
plt.show()

#Multivariate Analysis
sns.pairplot(df,huew="species",size=3)
-----------------------
Feature Importance
(select KBest alsocalled as information gain)
-----------------------
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
### Apply SelectKBest Algorithm
ordered_rank_features=SelectKBest(score_func=chi2,k=20)
ordered_feature=ordered_rank_features.fit(X,y)

dfscores=pd.DataFrame(ordered_feature.scores_,columns=["Score"])
dfcolumns=pd.DataFrame(X.columns)
features_rank=pd.concat([dfcolumns,dfscores],axis=1)
features_rank.columns=['Features','Score']
features_rank
:
features_rank.nlargest(10,'Score')

2nd Method
--------
from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
model=ExtraTreesClassifier()
model.fit(X,y)
ExtraTreesClassifier()
print(model.feature_importances_)
ranked_features=pd.Series(model.feature_importances_,index=X.columns)
ranked_features.nlargest(10).plot(kind='barh')
plt.show()

-------------------------------------
from sklearn.ensemble import ExtraTreesRegressor
import matplotlib.pyplot as plt
model=ExtraTreesRegressor()
model.fit(X,y)
ExtraTreesRegressor(random_state=101)
print(model.feature_importances_)
pd.Series(model.feature_importances_, index = X.columns).nlargest(5).plot(kind = 'barh')
plt.show()

3rd Method correlation
----------------------
df.corr()

import seaborn as sns
corr=df.iloc[:,:-1].corr()
top_features=corr.index
plt.figure(figsize=(20,20))
sns.heatmap(df[top_features].corr(),annot=True)


#### Remove The correlated
threshold=0.8
# find and remove correlated features
def correlation(dataset, threshold):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
    return col_corr

correlation(df.iloc[:,:-1],threshold)
set()

Information Gain¶
from sklearn.feature_selection import mutual_info_classif
mutual_info=mutual_info_classif(X,y)
mutual_data=pd.Series(mutual_info,index=X.columns)
mutual_data.sort_values(ascending=False)
Correlation
